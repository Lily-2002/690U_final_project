{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install umap-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:55:24.135142Z","iopub.execute_input":"2025-05-04T20:55:24.135511Z","iopub.status.idle":"2025-05-04T20:55:28.312729Z","shell.execute_reply.started":"2025-05-04T20:55:24.135481Z","shell.execute_reply":"2025-05-04T20:55:28.311383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix # Added confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport time\nimport matplotlib.pyplot as plt # Added for plotting\nimport seaborn as sns          # Added for heatmap\nimport umap.umap_ as umap # Added for UMAP dimensionality reduction\n\n\n# --- Configuration ---\nKMER_SIZE = 7\nMAX_FEATURES = None\nSVM_MAX_ITER = 30000 # Increased from previous\nGRID_SEARCH_CV_FOLDS = 3\nN_JOBS = -1\n# Config for visualizations\nN_TOP_FEATURES = 15 # Number of top/bottom features to show per class\nUMAP_N_NEIGHBORS = 15 # UMAP parameter (adjust based on data)\nUMAP_MIN_DIST = 0.1   # UMAP parameter (adjust based on data)\nUMAP_RANDOM_STATE = 42\nVISUALIZATION_SAMPLE_SIZE = 10000 # Reduce data size for faster UMAP/plotting if needed, set to None for full data\n\n# --- Step 1: Load data ---\nprint(\"Loading dataset...\")\nstart_time = time.time()\n\nsplits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\nDATASET_NAME = \"tattabio/ec_classification_dna\"\ndf_train = pd.read_parquet(f\"hf://datasets/{DATASET_NAME}/\" + splits[\"train\"])\ndf_test = pd.read_parquet(f\"hf://datasets/{DATASET_NAME}/\" + splits[\"test\"])\n\nprint(f\"Data loading took {time.time() - start_time:.2f} seconds.\")\nprint(f\"Train shape: {df_train.shape}, Test shape: {df_test.shape}\")\n\n# --- Step 2: Preprocess sequences ---\nprint(f\"\\nGenerating {KMER_SIZE}-mers...\")\nstart_time = time.time()\ndef kmerize_sequence(seq, k):\n    if pd.isna(seq) or len(seq) < k: return \"\"\n    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n\ndf_train['kmers'] = df_train['Sequence'].apply(lambda x: kmerize_sequence(x, KMER_SIZE))\ndf_test['kmers'] = df_test['Sequence'].apply(lambda x: kmerize_sequence(x, KMER_SIZE))\nprint(f\"K-mer generation took {time.time() - start_time:.2f} seconds.\")\n\n# --- Step 3: Feature extraction ---\nprint(f\"\\nExtracting features using TF-IDF (max_features={MAX_FEATURES})...\")\nstart_time = time.time()\nvectorizer = TfidfVectorizer(analyzer='word', max_features=MAX_FEATURES, ngram_range=(1, 1))\nX_train = vectorizer.fit_transform(df_train['kmers'])\nX_test = vectorizer.transform(df_test['kmers'])\nprint(f\"TF-IDF vectorization took {time.time() - start_time:.2f} seconds.\")\nprint(f\"Train feature matrix shape: {X_train.shape}\")\nprint(f\"Test feature matrix shape: {X_test.shape}\")\n\n# --- Step 4: Encode labels ---\nprint(\"\\nEncoding labels...\")\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(df_train['Label'])\ny_test = label_encoder.transform(df_test['Label'])\nnum_classes = len(label_encoder.classes_)\nprint(f\"Number of classes: {num_classes}\")\n\n# --- Step 5: Train Best Model (Skipping GridSearch for simplicity if C=130 is known best) ---\n# Using C=130 directly as found previously. Uncomment GridSearch if needed.\nbest_c = 130\nprint(f\"\\nTraining LinearSVC with C={best_c}...\")\nstart_time = time.time()\nbest_model = LinearSVC(\n    C=best_c,\n    class_weight='balanced',\n    dual=False,\n    max_iter=SVM_MAX_ITER,\n    random_state=42\n)\nbest_model.fit(X_train, y_train)\nprint(f\"Model training took {time.time() - start_time:.2f} seconds.\")\n\n# --- Step 6: Evaluate on test set ---\nprint(\"\\nEvaluating model on the test set...\")\ny_pred = best_model.predict(X_test)\nf1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\nf1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\nprint(f\"Final F1 Score on Test Set (macro): {f1_macro:.4f}\")\nprint(f\"Final F1 Score on Test Set (weighted): {f1_weighted:.4f}\")\n\n# --- Step 7: Visualization ---\nprint(\"\\n--- Starting Visualizations ---\")\n\n# 7.1: UMAP Projection of Training Data (on a subset for speed)\nif umap:\n    print(f\"\\nGenerating UMAP plot (sampling {VISUALIZATION_SAMPLE_SIZE or 'all'} points)...\")\n    start_time_umap = time.time()\n\n    # Subsample data if needed\n    if VISUALIZATION_SAMPLE_SIZE and VISUALIZATION_SAMPLE_SIZE < X_train.shape[0]:\n        sample_indices = np.random.choice(X_train.shape[0], VISUALIZATION_SAMPLE_SIZE, replace=False)\n        X_train_sample = X_train[sample_indices]\n        y_train_sample = y_train[sample_indices]\n    else:\n        X_train_sample = X_train\n        y_train_sample = y_train\n\n    try:\n        reducer = umap.UMAP(\n            n_neighbors=UMAP_N_NEIGHBORS,\n            min_dist=UMAP_MIN_DIST,\n            n_components=2,\n            metric='cosine', # Cosine is often good for TF-IDF\n            random_state=UMAP_RANDOM_STATE,\n            verbose=False\n        )\n        embedding = reducer.fit_transform(X_train_sample)\n\n        plt.figure(figsize=(12, 10))\n        # Use a colormap suitable for many categories, like 'viridis' or 'tab20'/'tab20b' but they repeat\n        # Scatter plot with small points and some transparency might be best\n        scatter = plt.scatter(\n            embedding[:, 0],\n            embedding[:, 1],\n            c=y_train_sample,\n            cmap='viridis', # 'tab20' or 'nipy_spectral' might also work\n            s=1, # Small points\n            alpha=0.5 # Transparency\n        )\n        plt.title(f'UMAP projection of TF-IDF features ({VISUALIZATION_SAMPLE_SIZE or \"All\"} samples)', fontsize=16)\n        plt.xlabel(\"UMAP Component 1\")\n        plt.ylabel(\"UMAP Component 2\")\n        # Add colorbar only if number of classes is manageable, otherwise it's useless\n        if num_classes <= 30:\n             plt.colorbar(scatter, label='Encoded Class Label')\n        else:\n             plt.text(0.95, 0.01, f'{num_classes} classes present',\n                      verticalalignment='bottom', horizontalalignment='right',\n                      transform=plt.gca().transAxes, fontsize=10)\n        plt.grid(True, linestyle='--', alpha=0.5)\n        print(f\"UMAP calculation and plotting took {time.time() - start_time_umap:.2f} seconds.\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"Could not generate UMAP plot: {e}\")\nelse:\n    print(\"\\nUMAP library not installed. Skipping UMAP plot.\")\n\n\n# 7.2: Confusion Matrix\nprint(\"\\nGenerating Confusion Matrix heatmap...\")\nstart_time_cm = time.time()\ntry:\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(12, 10)) # Adjust size as needed\n\n    # Normalize the confusion matrix to show percentages if desired\n    # cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    # sns.heatmap(cm_normalized, annot=False, cmap='viridis') # Use normalized version\n\n    # Plotting the raw counts - may be hard to read if large\n    # Decide whether to annotate based on size\n    annotate_cm = cm.shape[0] <= 30 # Only annotate if fewer than ~30 classes\n    sns.heatmap(cm, annot=annotate_cm, fmt='d', cmap='viridis', cbar=True) # fmt='d' for integers\n\n    plt.title('Confusion Matrix (True vs. Predicted Labels)', fontsize=16)\n    plt.ylabel('Actual Labels')\n    plt.xlabel('Predicted Labels')\n    # Ticks are automatically handled but might be dense if many classes\n    # plt.xticks(ticks=np.arange(len(label_encoder.classes_))+0.5, labels=label_encoder.classes_, rotation=90)\n    # plt.yticks(ticks=np.arange(len(label_encoder.classes_))+0.5, labels=label_encoder.classes_, rotation=0)\n    print(f\"Confusion Matrix generation took {time.time() - start_time_cm:.2f} seconds.\")\n    plt.show()\nexcept Exception as e:\n    print(f\"Could not generate Confusion Matrix plot: {e}\")\n\n\n# 7.3: Feature Importance (Top/Bottom k-mers for selected classes)\nprint(\"\\nCalculating Feature Importance (Top/Bottom k-mers)...\")\ntry:\n    coefficients = best_model.coef_\n    feature_names = np.array(vectorizer.get_feature_names_out())\n\n    # Select a few classes to inspect (e.g., first 3 or specific known classes)\n    classes_to_inspect_idx = np.arange(min(5, num_classes)) # Look at first 5 classes\n    classes_to_inspect_names = label_encoder.inverse_transform(classes_to_inspect_idx)\n\n    print(f\"\\n--- Top {N_TOP_FEATURES} Positive/Negative k-mers for Selected Classes ---\")\n    for i, class_name in zip(classes_to_inspect_idx, classes_to_inspect_names):\n        class_coef = coefficients[i]\n        top_positive_indices = np.argsort(class_coef)[-N_TOP_FEATURES:]\n        top_negative_indices = np.argsort(class_coef)[:N_TOP_FEATURES]\n\n        top_positive_features = feature_names[top_positive_indices]\n        top_positive_scores = class_coef[top_positive_indices]\n        top_negative_features = feature_names[top_negative_indices]\n        top_negative_scores = class_coef[top_negative_indices]\n\n        print(f\"\\nClass: {class_name} (Index: {i})\")\n        print(f\"  Top {N_TOP_FEATURES} Positive Features (Most Indicative):\")\n        for feature, score in zip(reversed(top_positive_features), reversed(top_positive_scores)):\n            print(f\"    '{feature}': {score:.4f}\")\n        print(f\"  Top {N_TOP_FEATURES} Negative Features (Most Contra-Indicative):\")\n        for feature, score in zip(top_negative_features, top_negative_scores):\n            print(f\"    '{feature}': {score:.4f}\")\n\nexcept Exception as e:\n    print(f\"Could not calculate feature importance: {e}\")\n\n\nprint(\"\\n--- End of Visualizations ---\")\nprint(\"--- End of Script ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:29:22.254409Z","iopub.execute_input":"2025-05-04T18:29:22.254808Z","iopub.status.idle":"2025-05-04T18:31:32.806487Z","shell.execute_reply.started":"2025-05-04T18:29:22.254775Z","shell.execute_reply":"2025-05-04T18:31:32.805606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport warnings\nfrom tqdm.auto import tqdm # For progress bar\nimport gc # Garbage collection\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n\n# --- Configuration ---\nMAX_SEQUENCE_LENGTH = 512 # Choose max length - smaller reduces dimensionality\nSVM_MAX_ITER = 3000       # Iterations for LinearSVC\nGRID_SEARCH_CV_FOLDS = 3\nN_JOBS = -1\n\n# One-Hot Encoding Mapping\n# Using floats for consistency with potential downstream processing\nONE_HOT_MAP = {\n    'A': np.array([1., 0., 0., 0.]),\n    'C': np.array([0., 1., 0., 0.]),\n    'G': np.array([0., 0., 1., 0.]),\n    'T': np.array([0., 0., 0., 1.]),\n    'N': np.array([0., 0., 0., 0.]) # Map N to zero vector\n}\nUNKNOWN_CHAR_VECTOR = np.array([0., 0., 0., 0.])\nPADDING_VECTOR = np.array([0., 0., 0., 0.]) # Padding represented by zeros\n\n# --- Step 1: Load data ---\nprint(\"Loading dataset...\")\nstart_time = time.time()\ntry:\n    splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n    DATASET_NAME = \"tattabio/ec_classification_dna\"\n    df_train = pd.read_parquet(f\"hf://datasets/{DATASET_NAME}/\" + splits[\"train\"])\n    df_test = pd.read_parquet(f\"hf://datasets/{DATASET_NAME}/\" + splits[\"test\"])\nexcept Exception as e:\n    print(f\"Error loading data from {DATASET_NAME}: {e}\")\n    exit()\nprint(f\"Data loading took {time.time() - start_time:.2f} seconds.\")\nprint(f\"Train shape: {df_train.shape}, Test shape: {df_test.shape}\")\n\n# Drop rows with missing sequences\ndf_train.dropna(subset=['Sequence'], inplace=True)\ndf_test.dropna(subset=['Sequence'], inplace=True)\n\n\n# --- Step 2: One-Hot Encoding Function ---\ndef one_hot_encode_sequence(sequence, max_len):\n    \"\"\"\n    Encodes a DNA sequence using one-hot mapping and pads/truncates\n    to a fixed length, then flattens.\n    \"\"\"\n    if pd.isna(sequence):\n        sequence = \"\"\n\n    # Ensure uppercase\n    sequence = sequence.upper()\n\n    encoded_vectors = []\n    seq_len = len(sequence)\n\n    for i in range(max_len):\n        if i < seq_len:\n            base = sequence[i]\n            # Get the vector or use unknown vector\n            vector = ONE_HOT_MAP.get(base, UNKNOWN_CHAR_VECTOR)\n        else:\n            # Pad with padding vector\n            vector = PADDING_VECTOR\n        encoded_vectors.append(vector)\n\n    # Flatten the list of vectors into a single 1D array\n    # Shape will be (max_len * 4,)\n    return np.array(encoded_vectors).flatten()\n\n# --- Step 3: Apply Encoding ---\nprint(f\"\\nApplying One-Hot Encoding (max_len={MAX_SEQUENCE_LENGTH})...\")\nstart_time = time.time()\n\n# Apply the function to each sequence and stack into a numpy array\n# This will create a large dense array (num_samples, max_len * 4)\nX_train_list = [one_hot_encode_sequence(seq, MAX_SEQUENCE_LENGTH) for seq in tqdm(df_train['Sequence'], desc=\"Encoding Train\")]\nX_train = np.vstack(X_train_list)\n\nX_test_list = [one_hot_encode_sequence(seq, MAX_SEQUENCE_LENGTH) for seq in tqdm(df_test['Sequence'], desc=\"Encoding Test\")]\nX_test = np.vstack(X_test_list)\n\nprint(f\"One-Hot Encoding took {time.time() - start_time:.2f} seconds.\")\nprint(f\"Train feature matrix shape: {X_train.shape}\") # Should be (num_samples, max_len * 4)\nprint(f\"Test feature matrix shape: {X_test.shape}\")\n\n# Clean up intermediate lists to save memory\ndel X_train_list, X_test_list\ngc.collect()\n\n# --- Step 4: Encode labels ---\nprint(\"\\nEncoding labels...\")\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(df_train['Label'])\ny_test = label_encoder.transform(df_test['Label'])\nnum_classes = len(label_encoder.classes_)\nprint(f\"Number of classes: {num_classes}\")\n\n# --- Step 5: Grid Search for best LinearSVM ---\n# LinearSVC should still work, but the high dimensionality might affect optimal C\n# Check n_features vs n_samples to inform dual setting.\nn_features = X_train.shape[1]\nn_samples = X_train.shape[0]\n# Set dual based on recommendation (False if n_samples > n_features)\ndual_setting = False if n_samples > n_features else True\nprint(f\"\\nUsing dual={dual_setting} for LinearSVC (n_samples={n_samples}, n_features={n_features})\")\n\nprint(\"\\nStarting Grid Search for LinearSVC on One-Hot Features...\")\nstart_time = time.time()\n\nparam_grid = {\n    'C': [0.025,0.026,0.027,0.028,0.029], # Test a range for C\n}\n\nsvm = LinearSVC(\n    class_weight='balanced',\n    dual=dual_setting,\n    max_iter=SVM_MAX_ITER,\n    random_state=42\n    )\n\ngrid_search = GridSearchCV(\n    svm,\n    param_grid,\n    cv=GRID_SEARCH_CV_FOLDS,\n    scoring='f1_macro',\n    verbose=2,\n    n_jobs=N_JOBS\n    )\n\n# Fit on the training data\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Grid Search took {time.time() - start_time:.2f} seconds.\")\nprint(\"Best Parameters found by Grid Search:\", grid_search.best_params_)\nprint(f\"Best F1 score (macro) during CV: {grid_search.best_score_:.4f}\")\n\n# --- Step 6: Evaluate on test set ---\nprint(\"\\nEvaluating best model on the test set...\")\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Calculate and print metrics\nf1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\nf1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n\nprint(f\"\\n--- Final Results (One-Hot Encoding + LinearSVC) ---\")\nprint(f\"Max Sequence Length: {MAX_SEQUENCE_LENGTH}\")\nprint(f\"Feature Dimension: {X_train.shape[1]}\")\nprint(f\"Best C: {grid_search.best_params_['C']}\")\nprint(\"-\" * 30)\nprint(f\"Final F1 Score on Test Set (macro): {f1_macro:.4f}\")\nprint(f\"Final F1 Score on Test Set (weighted): {f1_weighted:.4f}\")\nprint(\"-\" * 30)\n\nprint(\"\\nClassification Report on Test Set:\\n\")\ntry:\n    # (Optional: Add subset reporting logic if needed)\n    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))\nexcept Exception as report_e:\n    print(f\"Could not generate full classification report: {report_e}\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\nprint(\"--- End of Script ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:15:22.050172Z","iopub.execute_input":"2025-05-04T19:15:22.050535Z","iopub.status.idle":"2025-05-04T19:15:30.383370Z","shell.execute_reply.started":"2025-05-04T19:15:22.050511Z","shell.execute_reply":"2025-05-04T19:15:30.382149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV # Added GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import LinearSVC # Changed import\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    f1_score\n)\nfrom itertools import product\nimport time\nfrom tqdm.auto import tqdm # Added for progress bar\nimport warnings\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n\n# --- Configuration ---\nKMER_SIZE = 5      # K-mer length \nSVM_MAX_ITER = 50000 # Max iterations for LinearSVC convergence\nGRID_SEARCH_CV_FOLDS = 3\nN_JOBS = -1 # Use all available CPU cores\n\n# --- Encoding Function (Renamed for Accuracy) ---\ndef kmer_frequency_encode(sequences, k=3):\n    \"\"\"\n    Encode DNA sequences using normalized k-mer frequencies (KNC).\n    \"\"\"\n    print(f\"Generating all possible {k}-mers...\")\n    nucleotides = ['A', 'C', 'G', 'T']\n    # Handle k=0 or negative k gracefully\n    if k <= 0:\n        return np.array([[] for _ in sequences]) # Return empty features\n        \n    try:\n        kmers = [''.join(p) for p in product(nucleotides, repeat=k)]\n    except OverflowError: # Handle cases where 4^k is extremely large\n        print(f\"Error: k={k} results in too many k-mers (4^{k}). Choose a smaller k.\")\n        return None # Indicate failure\n        \n    kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n    num_kmers = len(kmers)\n    features = []\n    print(f\"Encoding {len(sequences)} sequences...\")\n\n    for seq in tqdm(sequences, desc=f\"Encoding {k}-mers\"):\n        counts = np.zeros(num_kmers, dtype=float) # Use float for division later\n        if pd.isna(seq) or len(seq) < k:\n            features.append(counts) # Append zero vector for invalid sequences\n            continue\n            \n        seq = seq.upper()\n        total_valid_kmers = 0\n        for i in range(len(seq) - k + 1):\n            kmer = seq[i:i + k]\n            # More robust check for valid k-mer characters\n            idx = kmer_index.get(kmer) # Use .get for faster lookup and handling of non-ACGT k-mers\n            if idx is not None:\n                counts[idx] += 1\n                total_valid_kmers += 1\n                \n        # Normalize counts\n        if total_valid_kmers > 0:\n            counts /= total_valid_kmers # In-place division\n            \n        features.append(counts)\n\n    return np.array(features)\n\n\ndef main():\n    # --- Step 1: Load data ---\n    print(\"Loading dataset...\")\n    start_time_load = time.time()\n    splits = {\n        'train': 'data/train-00000-of-00001.parquet',\n        'test': 'data/test-00000-of-00001.parquet'\n    }\n    DATASET_NAME = \"tattabio/ec_classification_dna\"\n    try:\n        df_train = pd.read_parquet(f\"hf://datasets/{DATASET_NAME}/\" + splits[\"train\"])\n        df_test = pd.read_parquet(f\"hf://datasets/{DATASET_NAME}/\" + splits[\"test\"])\n    except Exception as e:\n        print(f\"Error loading data from {DATASET_NAME}: {e}\")\n        exit()\n    print(f\"Data loading took {time.time() - start_time_load:.2f} seconds.\")\n    print(f\"Train shape: {df_train.shape}, Test shape: {df_test.shape}\")\n    \n    # Drop rows with missing sequences\n    df_train.dropna(subset=['Sequence'], inplace=True)\n    df_test.dropna(subset=['Sequence'], inplace=True)\n\n    # --- Step 2: Encode Sequences ---\n    print(f\"\\nEncoding sequences using {KMER_SIZE}-mer frequencies (KNC)...\")\n    start_time_encode = time.time()\n    X_train = kmer_frequency_encode(df_train['Sequence'].tolist(), k=KMER_SIZE)\n    X_test = kmer_frequency_encode(df_test['Sequence'].tolist(), k=KMER_SIZE)\n    \n    # Handle potential encoding failure (e.g., k too large)\n    if X_train is None or X_test is None:\n        print(\"Exiting due to encoding error.\")\n        exit()\n        \n    print(f\"Encoding completed in {time.time() - start_time_encode:.2f} seconds.\")\n    print(f\"Train feature matrix shape: {X_train.shape}\")\n    print(f\"Test feature matrix shape: {X_test.shape}\")\n    \n    # Check if encoding produced valid features\n    if X_train.shape[1] == 0:\n        print(\"Error: Encoding resulted in zero features. Check k-mer size and sequences.\")\n        exit()\n\n    # --- Step 3: Encode Labels ---\n    print(\"\\nEncoding labels...\")\n    le = LabelEncoder()\n    y_train = le.fit_transform(df_train['Label'])\n    y_test = le.transform(df_test['Label'])\n    print(f\"Number of classes: {len(le.classes_)}\")\n\n    # --- Step 4: Grid Search for best LinearSVC ---\n    # Determine dual setting\n    n_samples, n_features = X_train.shape\n    dual_setting = False if n_samples > n_features else True\n    print(f\"\\nUsing dual={dual_setting} for LinearSVC (n_samples={n_samples}, n_features={n_features})\")\n    \n    print(\"\\nStarting Grid Search for LinearSVC...\")\n    start_time_grid = time.time()\n    \n    param_grid = {\n        'C': [144,145,146], # Example range for regularization strength\n    }\n\n    svm = LinearSVC(\n        class_weight='balanced',\n        dual=dual_setting,\n        max_iter=SVM_MAX_ITER,\n        random_state=42\n    )\n\n    grid_search = GridSearchCV(\n        svm,\n        param_grid,\n        cv=GRID_SEARCH_CV_FOLDS,\n        scoring='f1_macro', # Use macro F1 for multi-class imbalance\n        verbose=2,\n        n_jobs=N_JOBS # Use all available cores\n    )\n\n    # Fit GridSearchCV on the training data\n    grid_search.fit(X_train, y_train)\n\n    print(f\"Grid Search took {time.time() - start_time_grid:.2f} seconds.\")\n    print(\"Best Parameters found by Grid Search:\", grid_search.best_params_)\n    print(f\"Best F1 score (macro) during CV: {grid_search.best_score_:.4f}\")\n\n    # --- Step 5: Evaluate on test set ---\n    print(\"\\nEvaluating best model on the test set...\")\n    best_model = grid_search.best_estimator_ # Already refitted on the whole train set\n    y_pred = best_model.predict(X_test)\n\n    # Calculate and print metrics\n    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n    \n    print(f\"\\n--- Final Results (KNC k={KMER_SIZE} + LinearSVC) ---\")\n    print(f\"Feature Dimension: {X_train.shape[1]}\")\n    print(f\"Best C found: {grid_search.best_params_['C']}\")\n    print(\"-\" * 30)\n    print(f\"Final F1 Score on Test Set (macro): {f1_macro:.4f}\")\n    print(f\"Final F1 Score on Test Set (weighted): {f1_weighted:.4f}\")\n    print(\"-\" * 30)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:24:14.247613Z","iopub.execute_input":"2025-05-04T20:24:14.248759Z","iopub.status.idle":"2025-05-04T20:34:28.398192Z","shell.execute_reply.started":"2025-05-04T20:24:14.248717Z","shell.execute_reply":"2025-05-04T20:34:28.396965Z"}},"outputs":[],"execution_count":null}]}